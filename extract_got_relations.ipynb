{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LLM to extract relations in a text\n",
    "\n",
    "For this session you either need Ollama installed (and running) together with a local model, or an account set up on OpenAI together with an API key.  \n",
    "  \n",
    "**Ollama**  \n",
    "You can find instructions on how to install Ollama [here](https://ollama.com).  \n",
    "I will use a model named `llama3:8b-instruct-q5_K_M` in this notebook. If your computer is less powerful, you might want to use a smaller model, like `3.8b-mini-4k-instruct-q4_1`.   \n",
    "Install a model by running `ollama run <model>` in your terminal. Beware that the models are big files so it might take a while to download them, especially if you have a slow internet connection.  \n",
    "  \n",
    "**OpenAI**  \n",
    "You can sign up for an account on OpenAI [here](https://platform.openai.com/signup).  \n",
    "After you have an account, you can find your API key [here](https://platform.openai.com/account/api-keys).  \n",
    "  \n",
    "The first code block below is a class that will set up an LLM connection for you, either with Ollama or OpenAI. This is so that we after that can use the same code, no matter which service you are using.  \n",
    "  \n",
    "Install the *ollama package* and *openai package* a code block like:  \n",
    "```\n",
    "%pip install ollama\n",
    "%pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self, OpenAI_key=False, model=None, temperature=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            OpenAI_key (str, optional): If you provide a key OpenAI will be used. Defaults to False.\n",
    "            model (str, optional): The model to use. Defaults to None.\n",
    "            temperature (int, optional): The temperature for generating text. Defaults to 0.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.temeprature = temperature\n",
    "        if OpenAI_key:\n",
    "            from openai import OpenAI\n",
    "\n",
    "            self.llm = OpenAI\n",
    "            self.client = OpenAI(api_key=OpenAI_key)\n",
    "            self.openai = True\n",
    "            self.ollama = False\n",
    "            if not model:\n",
    "                self.model = \"gpt-3.5-turbo\"\n",
    "\n",
    "        # For use with Ollama\n",
    "        else:\n",
    "            import ollama\n",
    "            self.llm = ollama\n",
    "            self.ollama = True\n",
    "            self.openai = False\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        if self.openai:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                model=self.model,\n",
    "            )\n",
    "            answer = chat_completion.choices[0].message.content\n",
    "        if self.ollama:\n",
    "            answer = self.llm.generate(\n",
    "                prompt=prompt, model=self.model, options={\"temperature\": self.temeprature}\n",
    "            )[\"response\"]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the LLM class\n",
    "llm = LLM(model='llama3:8b-instruct-q5_K_M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the text\n",
    "1. Import the text.\n",
    "2. Split it into episodes.\n",
    "3. Make a dictionary of the episodes like {episode_name: episode_text}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all relations from the chunks\n",
    "1. Define a function to extract relations\n",
    "2. Try out a working prompt.\n",
    "3. Loop throuh the chunks to create a list of relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get more information on every relation\n",
    "1. Define a function to extract information about a relation.\n",
    "2. Try out a working prompt.\n",
    "3. Loop though the relations to add intormation to each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare networkx\n",
    "1. Install networkx with ```%pip install networkx```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export a network file for use with Gephi\n",
    "1. Import the networkx module.\n",
    "2. Create the graph.\n",
    "3. Export a .gexf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
